# ðŸš€TensorFlow Developer Courses

This repository holds materials and notes from the DeepLearning.AI TensorFlow Developer Professional Certificate from Coursera.


# [AI for everyone Course by Andrew NG]([https://github.com/muntazirabidi/deeplearning_courses/tree/main/Introduction%20to%20Tensorflow](https://www.deeplearning.ai/courses/ai-for-everyone/))

**Notes**

- [Week 1: What is AI?](https://github.com/muntazirabidi/deeplearning_courses/blob/main/AI%20for%20Everyone/C1_W1.pdf)
- [Week 2: Building AI Projects](https://github.com/muntazirabidi/deeplearning_courses/blob/main/AI%20for%20Everyone/C1_W2.pdf)
- [Week 3: Building AI in Your Company](https://github.com/muntazirabidi/deeplearning_courses/blob/main/AI%20for%20Everyone/C1_W3.pdf)
- [Week 4: AI & Society](https://github.com/muntazirabidi/deeplearning_courses/blob/main/AI%20for%20Everyone/C1_W4.pdf)

# Course 1: [Introduction to Tensorflow for Artificial Intelligence, Machine Learning, and Deep Learning](https://github.com/muntazirabidi/deeplearning_courses/tree/main/Introduction%20to%20Tensorflow)

### Week 1
Learning objectives of the first week: 

**Learning Objectives**
- Monitor the accuracy of the housing price predictions
- Analyze housing price predictions that come from a single layer neural network
- Use TensorFlow to build a single layer neural network for fitting linear models

**Notes** 

The notes of the content covered in the first week can be accessed here: [first week notes](
https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/Notes/C1_W1.pdf)

**Jupyter Notebooks**

- [Hello World Assignment](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/C1_W1_Lab_1_hello_world_nn.ipynb)
- [Assignment 1: Housing Prices](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/C1W1_Assignment.ipynb)



### Week 2


- Use callback functions for tracking model loss and accuracy during training
- Make predictions on how the layer size affects network predictions and training speed
- Implement pixel value normalization to speed up network training
- Build a multilayer neural network for classifying the Fashion MNIST image dataset

**Notes**

The notes of the content covered in the second week can be accessed here: [second week notes](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/Notes/C1_W2.pdf)

**Responsible AI Practices**

The development of AI is creating new opportunities to improve the lives of people around the world, from business to healthcare to education. It is also raising new questions about the best way to build fairness, interpretability, privacy, and security into these systems.

[More to read ...](https://ai.google/responsibilities/responsible-ai-practices/)

- [Fairness](https://ai.google/responsibilities/responsible-ai-practices/?category=fairness)
- [Interpretability](https://ai.google/responsibilities/responsible-ai-practices/?category=interpretability)
- [Privacy](https://ai.google/responsibilities/responsible-ai-practices/?category=privacy)
- [Security](https://ai.google/responsibilities/responsible-ai-practices/?category=security)

**Jupyter Notebooks**

- [Get hands-on with computer vision](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/C1_W2_Lab_1_beyond_hello_world.ipynb)
- [Using Callbacks to Control Training](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/C1_W2_Lab_2_callbacks.ipynb)
- [Assignment 2: Implementing Callbacks in TensorFlow using the MNIST Dataset](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/C1W2_Assignment.ipynb)


### Week 3

**Learning Objectives**

- Use callback functions to interrupt training after meeting a threshold accuracy
- Test the effect of adding convolution and MaxPooling to the neural network for classifying Fashion MNIST images on classification accuracy
- Explain and visualize how convolution and MaxPooling aid in image classification tasks

**Notes**

The notes of the content covered in the third week can be accessed here: [third week notes](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/Notes/C1_W3.pdf)

- [tf.keras.layers.Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)
- [tf.keras.layers.MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)
- [To learn more about convolutions ... ](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF)

**Jupyter Notebooks**
- [Improving Computer Vision Accuracy using Convolutions](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/C1_W3_Lab_1_improving_accuracy_using_convolutions.ipynb)
- [Exploring Convolutions](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/C1_W3_Lab_2_exploring_convolutions.ipynb)
- [Assignment 3: Improve MNIST with Convolutions](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/C1W3_Assignment.ipynb)


### Week 4

**Learning Objectives**

- Reflect on the possible shortcomings of your binary classification model implementation
- Execute image preprocessing with the Keras ImageDataGenerator functionality
- Carry out real life image classification by leveraging a multilayer neural network for binary classification

**Notes**

The notes of the content covered in the fourth week can be accessed here: [fourth week notes](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/Notes/C1_W4.pdf)

- [Binary Cross Entropy](https://gombru.github.io/2018/05/23/cross_entropy_loss/)
- [RMSprop](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/RMSprop)
- [Binary Classification video](https://www.youtube.com/watch?v=eqEc66RFY0I&t=6s)
- [Stochastic Gradient Descent](https://developers.google.com/machine-learning/glossary/#SGD)

**Jupyter Notebooks**
- [Training with ImageDataGenerator](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/C1_W4_Lab_1_image_generator_no_validation.ipynb)
- [ImageDataGenerator with a Validation Set](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/C1_W4_Lab_2_image_generator_with_validation.ipynb)
- [Effect of Compacted Images in Training](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/C1_W4_Lab_3_compacted_images.ipynb)
- [Assignment 4: Handling Complex Images](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Introduction%20to%20Tensorflow/C1W4_Assignment.ipynb)


# Course 2: [Convolutional Neural Networks in TensorFlow](https://github.com/muntazirabidi/deeplearning_courses/tree/main/Convolutional%20Neural%20Networks%20in%20TensorFlow)

In this course we'll go deeper into using ConvNets will real-world data, and learn about techniques that you can use to improve your ConvNet performance, particularly when doing image classification! 

## Week 1

**Learning Objectives**

- Gain understanding about Kerasâ€™ utilities for pre-processing image data, in particular the ImageDataGenerator class
- Develop helper functions to move files around the filesystem so that they can be fed to the ImageDataGenerator
- Learn how to plot training and validation accuracies to evaluate model performance
- Build a classifier using convolutional neural networks for performing cats vs dogs classification

**Data**

[Dogs vs Cats Dataset from Kaagle](https://www.kaggle.com/competitions/dogs-vs-cats/data)

**Notes**

- [Some helper functions using `os` module](https://docs.python.org/3/library/os.html)


**Jupyter Notebooks**
- [Using more sophisticated images with Convolutional Neural Networks](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Convolutional%20Neural%20Networks%20in%20TensorFlow/Notebooks/C2_W1_Lab_1_cats_vs_dogs%20(1).ipynb)
- [Assignment1 : Using CNN's with the Cats vs Dogs Dataset](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Convolutional%20Neural%20Networks%20in%20TensorFlow/Notebooks/C2W1_Assignment.ipynb)



## Week 2

Overfitting is simply the concept of being over specialized in training -- namely that your model is very good at classifying what it is trained for, but not so good at classifying things that it hasn't seen. In order to generalize your model more effectively, you will of course need a greater breadth of samples to train it on. That's not always possible, but a nice potential shortcut to this is **Image Augmentation**, where you tweak the training set to potentially increase the diversity of subjects it covers.

**Learning Objectives**

- Recognize the impact of adding image augmentation to the training process, particularly in time
- Demonstrate overfitting or lack of by plotting training and validation accuracies
- Familiarize with the ImageDataGenerator parameters used for carrying out image augmentation
- Learn how to mitigate overfitting by using data augmentation techniques

**Notes**

- [To know more about augmentation](https://keras.io/api/layers/preprocessing_layers/)
- Data augmentation can solve overfitting in Cats vs Dogs datasets. But applied the dataset of Humans vs Horses, it doesn't perform well. The reason could be that data augmentation do not revognise the features in the validation set. 

**Data**

- [Humans vs Horses : Training Set](https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip)
- [Humans vs Horses : Validation Set](https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip)

**Jupyter Notebooks**

- [Data Augmentation](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Convolutional%20Neural%20Networks%20in%20TensorFlow/Notebooks/C2_W2_Lab_1_cats_v_dogs_augmentation.ipynb)
- [Data Augmentation on the Horses or Humans Dataset](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Convolutional%20Neural%20Networks%20in%20TensorFlow/Notebooks/C2_W2_Lab_2_horses_v_humans_augmentation.ipynb)
- [Tackle Overfitting with Data Augmentation](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Convolutional%20Neural%20Networks%20in%20TensorFlow/Notebooks/C2W1_Assignment.ipynb)


## Week 3

**Learning Objective**

- Master the keras layer type known as dropout to avoid overfitting
- Achieve transfer learning in code using the keras API
- Code a model that implements Kerasâ€™ functional API instead of the commonly used Sequential model
- Learn how to freeze layers from an existing model to successfully implement transfer learning
- Explore the concept of transfer learning to use the convolutions learned by a different model from a larger dataset

**Notes**
- [Understanding Dropout](https://www.youtube.com/watch?v=ARq74QuavAo)
- [Transfer Learning and Dropout](https://www.tensorflow.org/tutorials/images/transfer_learning)
- [Inception architecture for Computer Vision](https://arxiv.org/abs/1512.00567)
- [Pretrained weights](https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5)

**Jupyter Notebooks**

- [Transfer Learning](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Convolutional%20Neural%20Networks%20in%20TensorFlow/Notebooks/C2_W3_Lab_1_transfer_learning.ipynb)
- [Assignment 3: Transfer Learning Using Inception Architecture](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Convolutional%20Neural%20Networks%20in%20TensorFlow/Notebooks/C2W3_Assignment.ipynb)


## Week 4

**Learning Objectives**

- Build a multiclass classifier for the Sign Language MNIST dataset
- Learn how to properly set up the ImageDataGenerator parameters and the model definition functions for multiclass classification
- Understand the difference between using actual image files vs images encoded in other formats and how this changes the methods available when using ImageDataGenerator
- Code a helper function to parse a raw CSV file which contains the information of the pixel values for the images used

**Data**

- [Data sets used in this course](https://laurencemoroney.com/datasets.html)
- [Sign Language MNIST](https://www.kaggle.com/datamunge/sign-language-mnist)


**Jupyter Notebooks**

- [Multi-class Classifier](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Convolutional%20Neural%20Networks%20in%20TensorFlow/Notebooks/C2_W4_Lab_1_multi_class_classifier.ipynb)
- [Assignment 4: Multi-class Classifier of Sign Languages](https://github.com/muntazirabidi/deeplearning_courses/blob/main/Convolutional%20Neural%20Networks%20in%20TensorFlow/Notebooks/C2W4_Assignment.ipynb)

## Summary
In this course we spent a lot of time on Convolutional Neural Networks: 

- Exploring how to use them with large datasets

- Taking advantage of augmentation, dropout, regularization, and transfer learning

- Looking at the coding considerations between binary or multi-class classification


# Course 3: [Natural Language Processing in TensorFlow](https://github.com/muntazirabidi/deeplearning_courses/tree/main/NLP%20in%20TensorFlow)

## Week 1

**Learning Objectives**

The first step in understanding sentiment in text, and in particular when training a neural network to do so is the tokenization of that text. This is the process of converting the text into numeric values, with a number representing a word or a character. We will learn about the Tokenizer and pad_sequences APIs in TensorFlow and how they can be used to prepare and encode text and sentences to get them ready for training neural networks!

**Data**

- [News headlines dataset for sarcasm detection: Kaagle](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection)
- [Download the Dataset](https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json)
- [BBC News Classification Dataset](http://mlg.ucd.ie/datasets/bbc.html)

**Jupyter Notebooks**

- [Tokenizer Basics](https://github.com/muntazirabidi/deeplearning_courses/blob/main/NLP%20in%20TensorFlow/Notebooks/C3_W1_Lab_1_tokenize_basic.ipynb)
- [Generating Sequences and Padding](https://github.com/muntazirabidi/deeplearning_courses/blob/main/NLP%20in%20TensorFlow/Notebooks/C3_W1_Lab_2_sequences_basic.ipynb)
- [Tokenizing the Sarcasm Dataset](https://github.com/muntazirabidi/deeplearning_courses/blob/main/NLP%20in%20TensorFlow/Notebooks/C3_W1_Lab_3_sarcasm.ipynb)
